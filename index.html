<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Anthony Simeonov</title>

    <meta name="author" content="Anthony Simeonov">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Anthony Simeonov</name>
                                    </p>
                                    <p>I am a Ph.D. student in the EECS department at MIT, advised by Professor <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>
                                        and Professor <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>. I'm interested in robot manipulation and physical interaction.

                                        I am fortunate to be supported by the <a href="https://www.nsfgrfp.org/">NSF GRFP</a>. 
                                    </p>
                                    <p>
                                        I received my S.M. in EECS from MIT and my B.S. in Mechanical Engineering from UC San Diego, where I
                                        worked with Professor <a href="https://yip.eng.ucsd.edu/">Michael Yip</a>. I have also spent time as a research intern at 
                                        <a href="https://la.disneyresearch.com/">Disney Research Los Angeles</a> and <a href="https://research.nvidia.com/labs/srl/">NVIDIA's Seattle Robotics Lab</a> 
                                    </p>
                                    <p align=center>
                                        <a href="mailto:asimeono@mit.edu">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=RNDrSGYAAAAJ&hl=en" target="_blank">Google Scholar</a>
                                        &nbsp/&nbsp
                                        <a href="https://www.linkedin.com/in/anthonysimeonov/" target="_blank">LinkedIn</a>
                                    </p>
                                    
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <img style="width:100%;max-width:100%" alt="profile photo" src="images/slab-ant-cropped (circle).jpg">
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm currently interested in machine learning for perception-driven planning and control, applied to problems in robot manipulation and physical interaction.
                                        The goal is to equip robots with <em>reusable</em> manipulation skills, through a combination of effective perceptual representations and reactive behaviors. 
                                        Previously, I have studied the design, modeling, and control of inherently compliant artificial muscle actuators.  
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Publications</heading>
                                    <p>
                                        <!-- I'm currently interested in machine learning for data-driven planning and control.
                                                            Previously, I have studied the use of compliant materials as robot actuators.   -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <!-- <img src='images/mugandshelffast.gif' width="100%" /> -->
                                    <img src='images/place_seq3.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2407.16677">
                                        <papertitle>From Imitation to Refinement - Residual RL for Precise Visual Assembly</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://ankile.com/"> Lars Ankile</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://idanshen.github.io/"> Idan Shenfeld</a>,
                                    <a href="https://marceltorne.github.io/"> Marcel Torne</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal</a>
                                    <br>
                                    <em>arXiv</em>, 2024 
                                    <br>
                                    <a href="https://residual-assembly.github.io/">project page</a> 
                                    <br>
                                    <p></p>
                                    <p>
                                    A BC + RL pipeline for precise manipulation tasks like assembly. Fine-tuning BC-trained policies with action chunking and diffusion de-noising requires specialized RL methods. We simplify such RL fine-tuning with simple residual policies trained with PPO.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <!-- <img src='images/mugandshelffast.gif' width="100%" /> -->
                                    <img src='images/mug_real_sim_rialto_fast.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2403.03949">
                                        <papertitle>Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://marceltorne.github.io/"> Marcel Torne</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://supersglzc.github.io/"> Zechu Li</a>,
                                    <a href="https://real-to-sim-to-real.github.io/RialTo/"> April Chan</a>,
                                    <a href="https://taochenshh.github.io/"> Tao Chen</a>,
                                    <a href="https://homes.cs.washington.edu/~abhgupta/"> Abhishek Gupta*</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal*</a>(*equal advising)
                                    <br>
                                    <em>RSS</em>, 2024 
                                    <br>
                                    <a href="https://real-to-sim-to-real.github.io/RialTo/">project page</a> 
                                    <br>
                                    <p></p>
                                    <p>
                                    Learning robust manipulation policies by combining real-world imitation learning and reinforcement learning in simulation. RL in sim uses digital twin assets that reflect the geometry, appearance, and kinematics of the target deployment scene.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <!-- <img src='images/mugandshelffast.gif' width="100%" /> -->
                                    <img src='images/juicer_quick_s2.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2404.03729">
                                        <papertitle>JUICER: Data-Efficient Imitation Learning for Robotic Assembly</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://ankile.com/"> Lars Ankile</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://idanshen.github.io/"> Idan Shenfeld</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal</a>
                                    <br>
                                    <em>IROS</em>, 2024 
                                    <br>
                                    <a href="https://imitation-juicer.github.io/">project page</a> 
                                    <br>
                                    <p></p>
                                    <p>
                                    A pipeline for learning image-based policies for precise, long-horizon assembly tasks from a small number of demonstrations by combining expressive policy architectures and various techniques for dataset expansion
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <!-- <img src='images/mugandshelffast.gif' width="100%" /> -->
                                    <img src='images/halp_pour.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2309.14321">
                                        <papertitle>Lifelong Robot Learning with Human Assisted Language Planners</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://sites.google.com/mit.edu/halp-robot-learning"> Meenal Parakh*</a>,
                                    <a href="https://sites.google.com/mit.edu/halp-robot-learning"> Alisha Fong*</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://taochenshh.github.io/"> Tao Chen</a>,
                                    <a href="https://homes.cs.washington.edu/~abhgupta/"> Abhishek Gupta</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal</a>(*equal contribution)
                                    <br>
                                    <em>ICRA</em>, 2024 
                                    <br>
                                    <a href="https://sites.google.com/mit.edu/halp-robot-learning">project page</a> 
                                    <br>
                                    <p></p>
                                    <p>
                                    An LLM-based task planner that can learn new skills opens doors for continual learning.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/rpdiff-real-sim-spedup-notext-notitlec.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2307.04751">
                                        <papertitle>Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://imankgoyal.github.io/"> Ankit Goyal*</a>,
                                    <a href="http://lucasmanuelli.com/"> Lucas Manuelli*</a>,
                                    <a href="https://yenchenlin.me/"> Lin Yen-Chen</a>,
                                    <a href="https://www.linkedin.com/in/alina-sarmiento/"> Alina Sarmiento</a>,
                                    <a href="https://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU"> Alberto Rodriguez</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/"> Pulkit Agrawal**</a>,
                                    <a href="https://homes.cs.washington.edu/~fox/"> Dieter Fox**</a>(*equal contribution, **equal advising)
                                    <br>
                                    <em>CoRL</em>, 2023 
                                    <br>
                                    <a href="https://anthonysimeonov.github.io/rpdiff-multi-modal/">project page</a> / <a href="https://github.com/anthonysimeonov/rpdiff">code</a>
                                    <br>
                                    <p></p>
                                    <p>
                                    Performing 6-DoF relational rearrangement between objects and scenes. Iterative pose de-noising via diffusion helps handle multi-modality and local scene cropping improves generalization.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/lndf.gif' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2302.03573">
                                        <papertitle>Local Neural Descriptor Fields: Locally Conditioned Object Representations for Manipulation</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://elchun.github.io/">Ethan Chun</a>,
                                    <a href="https://yilundu.github.io/">Yilun Du</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://people.csail.mit.edu/tlp/">Tom&aacute;s Lozano-P&eacute;rez</a>,
                                    <a href="https://people.csail.mit.edu/lpk/">Leslie Kaelbling</a>
                                    <br>
                                    <em>ICRA</em>, 2023 
                                    <br>
                                    <a href="https://elchun.github.io/lndf/">project page</a> / <a href="https://github.com/elchun/lndf_robot">code</a>
                                    <br>
                                    <p></p>
                                    <p>
					Local point cloud encoders help 3D neural descriptor fields generalize to more diverse global shapes and improve robustness to occlusions
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/rndf_bowl_bottle_notext.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2211.09786">
                                        <papertitle>SE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov*</strong>,
                                    <a href="https://yilundu.github.io/">Yilun Du*</a>,
                                    <a href="https://yenchenlin.me/">Lin Yen-Chen</a>,
                                    <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                                    <a href="https://people.csail.mit.edu/lpk/">Leslie P. Kaelbling</a>,
                                    <a href="https://people.csail.mit.edu/tlp/">Tom&aacute;s Lozano-P&eacute;rez</a>
                                    <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a> (*equal contribution)
                                    <br>
                                    <em>CoRL</em>, 2022 
                                    <br>
                                    <a href="https://anthonysimeonov.github.io/r-ndf/">project page</a> / <a href="https://github.com/anthonysimeonov/relational_ndf">code</a>
                                    <br>
                                    <p></p>
                                    <p>
					Applying 3D neural descriptor fields in a pairwise fashion, to enable relational rearrangement with pairs of unseen objects in arbitrary poses.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/mira-teaser.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2212.06088">
                                        <papertitle>MIRA: Mental Imagery for Robotic Affordances</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://yenchenlin.me/">Lin Yen-Chen</a>,
                                    <a href="https://www.peteflorence.com/">Pete Florence</a>,
                                    <a href="https://andyzeng.github.io/">Andy Zeng</a>,
                                    <a href="https://jonbarron.info/">Johnathon T. Barron</a>,
                                    <a href="https://yilundu.github.io/">Yilun Du</a>,
                                    <a href="https://people.csail.mit.edu/weichium/">Wei-Chiu Ma</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                                    <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>
                                    <br>
                                    <em>CoRL</em>, 2022 
                                    <br>
                                    <a href="https://yenchenlin.me/mira/">project page</a> 
                                    <br>
                                    <p></p>
                                    <p>
					NeRF enables synthesizing orthographic views from virtual camera poses, allowing search for 6-DoF placing actions represented by pixels in the rendered image (positions) and the camera pose used to generate the image (orientations)
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/mug_cut.gif' width="100%" />
                                    <!--<img src='images/mug_cut.gif' width="100%" />-->
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2112.05124">
                                        <papertitle>Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov*</strong>,
                                    <a href="https://yilundu.github.io/">Yilun Du*</a>,
                                    <a href="https://taiya.github.io/">Andrea Tagliasacchi</a>,
                                    <a href="http://web.mit.edu/cocosci/josh.html">Joshua Tenenbaum</a>,
                                    <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>
                                    <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal**</a>,
                                    <a href="https://www.vincentsitzmann.com/">Vincent Sitzmann**</a> (*equal contribution, order determined by coin flip. **equal advising)
                                    <br>
                                    <em>ICRA</em>, 2022 <!--(under review for <em>ICRA</em> 2022)-->
                                    <br>
                                    <a href="https://yilundu.github.io/ndf/">project page</a> / <a href="https://github.com/anthonysimeonov/ndf_robot">code</a>
                                    <br>
                                    <p></p>
                                    <p>
					A novel representation that models objects as 3D neural fields of descriptors. We apply this representation to enable pick-and-place on unseen objects in out-of-distribution poses from a small handful of demonstrations.
                                    </p>
                                    <!-- <p>
					A representation that models objects as 3D neural fields of descriptors that encode category-level correspondence and are SE(3)-equivariant by construction. We apply this representation to enable pick-and-place on unseen objects in out-of-distribution poses from a small handful of demonstrations.
                                    </p> -->
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/multistep_taller.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/abs/2011.08177">
                                        <papertitle>A Long Horizon Planning Framework for
                                        Manipulating Rigid Pointcloud Objects</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://yilundu.github.io/">Yilun Du</a>,
                                    <a href="http://people.csail.mit.edu/beomjoon/">Beomjoon Kim</a>,
                                    <a href="https://www.linkedin.com/in/francois-hogan-2b4025b6/">Francois Hogan</a>,
                                    <a href="http://web.mit.edu/cocosci/josh.html">Joshua Tenenbaum</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                                    <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>
                                    <br>
                                    <em>CoRL</em>, 2020
                                    <br>
                                    <a href="https://anthonysimeonov.github.io/rpo-planning-framework/">project page</a>
                                    <br>
                                    <p></p>
                                    <!-- <p>We train conditional generative models to map object point clouds to a distribution over the contact and subgoal parameters of a set of manipulation skills, 
                                        and integrate these samplers into a framework for planning multistep manipulation of unknown rigid objects.
                                    </p> -->
                                    <p>
                                        A framework for solving long-horizon planning problems involving manipulation of rigid objects that operates directly
                                        from a point-cloud observation
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/mpnet-planning-quad.png' width="100%"/>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1907.06013.pdf" id="MPNet_journal">
                                        <papertitle>Motion Planning Networks: Bridging the Gap Between
                                        Learning-based and Classical Motion Planners</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://scholar.google.com/citations?user=Lkrx2SkAAAAJ&hl=en">Ahmed Qureshi</a>,
                                    Yinglong Miao,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://yip.eng.ucsd.edu/">Michael C. Yip</a>
                                    <br>
                                    <em>TRO</em>, 2020 
                                    <br>
                                    <a href="https://sites.google.com/view/mpnet/home">project page</a>
                                    <br>
                                    <p></p>
                                    <p>By encoding raw observations of obstacle geometry into a latent representation, 
                                        we can distill the capabilities of an expensive planning algorithm 
                                        into a neural network, allowing significant speedups in finding near-optimal collision-free paths.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/baxter_mpnet_quad.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/1806.05767.pdf">
                                        <papertitle>Motion Planning Networks</papertitle>
                                    </a>
                                    <br>
                                    <a href="https://scholar.google.com/citations?user=Lkrx2SkAAAAJ&hl=en">Ahmed Qureshi</a>,
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://scholar.google.com/citations?user=juDRQNkAAAAJ&hl=en">Mayur J. Bency</a>,
                                    <a href="https://yip.eng.ucsd.edu/">Michael C. Yip</a>
                                    <br>
                                    <em>ICRA</em>, 2019
                                    <br>
                                    <a href="https://sites.google.com/view/mpnet/home">project page</a>
                                    <br>
                                    <p></p>
                                    <p>This paper is subsumed by <a href="#MPNet_journal">our journal paper</a>.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <!-- <img src='images/RAL18-bundle-examples-tri.png' width="100%" /> -->
                                    <img src='images/bundling-schematic-quad.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://97315462-8792-4096-8cb2-f015b393fa99.filesusr.com/ugd/e4c3d0_5d09f93d365447fca819af0fc4ad1a9a.pdf">
                                        <papertitle>Bundled super-coiled polymer artificial muscles: Design, characterization, and modeling</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://www.linkedin.com/in/taylor-henderson-565204133/">Taylor Henderson</a>,
                                    <a href="https://www.linkedin.com/in/zixuan-lan-498b46ab/">Zixuan Lan</a>,
                                    <a href="https://www.linkedin.com/in/guhansundar/">Guhan Sundar</a>,
                                    Adam Factor,
                                    <a href="https://www.unr.edu/me/people/jun-zhang"> Jun Zhang</a>,
                                    <a href="https://yip.eng.ucsd.edu/">Michael C. Yip</a>
                                    <br>
                                    <em>RA-L</em>, 2018
                                    <p></p>
                                    <p>Empirical study of the performance tradeoffs between different bundling techniques for thermally-driven, compliant artificial muscle actuators. 
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/stickman-sequence.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a
                                        href="https://s3-us-west-1.amazonaws.com/disneyresearch/wp-content/uploads/20180515182735/Stickman-Towards-a-Human-Scale-Acrobatic-Robot-Paper.pdf">
                                        <papertitle>Stickman: Towards a Human Scale Acrobatic Robot
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.linkedin.com/in/morganthomaspope/">Morgan T. Pope</a>, 
                                    Steven Christensen, 
                                    <!-- <a href="https://www.linkedin.com/in/steven-christensen-9b416640/">Steven Christensen</a>, -->
                                    <a href="https://scholar.google.com/citations?user=COZRMZUAAAAJ&hl=en">David Christensen</a>, 
                                    <strong>Anthony Simeonov</strong>, 
                                    <a href="https://www.linkedin.com/in/grant-imahara-53411b5/">Grant Imahara</a>, 
                                    <a href="http://www.mce.caltech.edu/people/gunter">Günter Niemeyer</a>
                                    <br>
                                    <em>ICRA</em>, 2018
                                    <p></p>
                                    <p>We present a two DOF robot that is launched from a gravity-driven pendulum and executes a variety of
                                    mid-air stunts. The robot uses a combination of onboard sensors to perform state estimation
                                    and inform actuator timing.
                                    </p>
                                    <p>
                                    This research is part of the basis of Imagineering's <a href="https://la.disneyresearch.com/stuntronics/">Stuntronics</a> project.
                                    </p>
                                </td>
                            </tr>                            

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/sms-3d-hysteresis-diagram.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a
                                        href="https://www.researchgate.net/profile/Jun_Zhang199/publication/322377486_Three-dimensional_Hysteresis_Compensation_Enhances_Accuracy_of_Robotic_Artificial_Muscles/links/5a58e6beaca2727d60815349/Three-dimensional-Hysteresis-Compensation-Enhances-Accuracy-of-Robotic-Artificial-Muscles.pdf">
                                        <papertitle>Three-dimensional hysteresis compensation enhances accuracy of robotic artificial muscles
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.unr.edu/me/people/jun-zhang"> Jun Zhang</a>, 
                                    <strong>Anthony Simeonov</strong>, 
                                    <a href="https://yip.eng.ucsd.edu/">Michael C. Yip</a>
                                    <br>
                                    <em>Smart Materials and Structures</em>, 2018
                                    <p></p>
                                    <p>
                                       A new modeling scheme for capturing the coupled hysteresis between tension, strain, 
                                       and input, which is used for improved control of artificial muscle actuators. 
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/scp-hysteresis-figs.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a
                                        href="https://97315462-8792-4096-8cb2-f015b393fa99.filesusr.com/ugd/e4c3d0_93f47f3fd74944bf97665a5c877bdce4.pdf">
                                        <papertitle>Modeling and inverse compensation of hysteresis in supercoiled polymer artificial muscles
                                        </papertitle>
                                    </a>
                                    <br>
                                    <a href="https://www.unr.edu/me/people/jun-zhang"> Jun Zhang</a>, 
                                    <a href="https://www.linkedin.com/in/kiyer95/">Kaushik Iyer</a>, 
                                    <strong>Anthony Simeonov</strong>, 
                                    <a href="https://yip.eng.ucsd.edu/">Michael C. Yip</a>
                                    <br>
                                    <em>RA-L</em>, 2017
                                    <p></p>
                                    <p>We propose three new models for characterizing the hysteresis present in super-coiled polymer artificial muscles,
                                        and demonstrate their use for accurate open-loop control.
                                    </p>
                                </td>
                            </tr>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Workshop Papers</heading>
                                    <p>
                                        <!-- I'm currently interested in machine learning for data-driven planning and control.
                                                                                Previously, I have studied the use of compliant materials as robot actuators.   -->
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                    
                            <tr>
                                <td style="padding:20px;width:25%;vertical-align:middle">
                                    <img src='images/multistep_taller.png' width="100%" />
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://rss2020vlrrm.github.io/papers/14_CameraReadySubmission_LearningManipSkills_RSS2020_VLRRM_CameraReady.pdf">
                                        <papertitle>Learning to Plan with Pointcloud Affordances for General-Purpose Dexterous Manipulation</papertitle>
                                    </a>
                                    <br>
                                    <strong>Anthony Simeonov</strong>,
                                    <a href="https://yilundu.github.io/">Yilun Du</a>,
                                    <a href="http://people.csail.mit.edu/beomjoon/">Beomjoon Kim</a>,
                                    <a href="https://www.linkedin.com/in/francois-hogan-2b4025b6/">Francois Hogan</a>,
                                    <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                                    <a href="http://meche.mit.edu/people/faculty/ALBERTOR@MIT.EDU">Alberto Rodriguez</a>,
                                    <br>
                                    <em>RSS 2020 Workshops on Visual Learning and Reasoning for Robotic Manipulation, and Learning in Task and Motion Planning</em>
                                    <br>
                                    <!-- <a href="">project page</a> -->
                                    <br>
                                    <p></p>
                                    <!-- <p>A framework that aims to achieve the best of TAMP and robot-learning for manipulating unmodeled rigid objects.
                                    </p> -->
                                </td>
                            </tr>
                    
                    </table>
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <a href="https://github.com/jonbarron/jonbarron_website">template</a>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                </td>
            </tr>
    </table>
</body>

</html>
